{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(home_dir)\n",
    "from src.evaluating.eval_simple import get_dcsm_scores, get_naive_clip_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "12.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)  # This shows the CUDA version PyTorch was compiled with\n",
    "print(torch.cuda.is_available())  # Checks if CUDA is available on your system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reproduce Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "above\n",
      "below\n",
      "without\n",
      "DCSM: tensor([6.3090, 6.1073, 3.3570, 2.5674, 4.1543, 2.0319]) \n",
      "CLIP: tensor([[0.3202, 0.3386, 0.3058, 0.3033, 0.2964, 0.2997]]) \n",
      "CLIP 32 tensor([[0.3310, 0.3449, 0.2956, 0.3008, 0.2946, 0.2944]])\n"
     ]
    }
   ],
   "source": [
    "images = [Image.open(os.path.join(home_dir, \"data\", f\"test_image1.jpg\")).convert(\"RGB\")]\n",
    "texts = [\"red circle blue triangle\", \"red triangle blue circle\", \"circle above triangle\", \"circle below triangle\", \"circle with triangle\", \"circle without triangle\"]\n",
    "# pretrain_model_dir = r\"D:\\temporary_data\\art_with_shapes\\Logical-CLIP\\src\\models\\patchnet_MANY26_spw_True_batchsize_8_lr_0.001_drop_0.1_lambda_0_singlew_Falsehalfneg_False_fix_objaverse_std_wneg_v5.pt\"\n",
    "pretrain_model_dir = r\"D:\\Ideal_CLIP\\Ideal-CLIP-DCSM-private\\pretrained_models\\dcsm_obja_ckpt.pt\"\n",
    "# os.path.join(\"pretrained_models\", \"dcsm_obja_ckpt.pt\")\n",
    "dcsm_out = get_dcsm_scores(pretrain_model_dir, images, texts)\n",
    "\n",
    "clip_out = get_naive_clip_scores(images,texts, 16) #change patch size between 16 & 32\n",
    "clip_out32 = get_naive_clip_scores(images,texts, 32) #change patch size between 16 & 32\n",
    "\n",
    "print(\"DCSM:\",dcsm_out.detach().cpu(), \"\\nCLIP:\",clip_out.detach().cpu(), \"\\nCLIP 32\", clip_out32.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right\n",
      "left\n",
      "without\n",
      "DCSM: tensor([2.8889, 2.7702, 1.8300, 1.7810, 4.9823, 3.1482]) \n",
      "CLIP: tensor([[0.2904, 0.2965, 0.3181, 0.3141, 0.3244, 0.3259]]) \n",
      "CLIP 32 tensor([[0.2878, 0.2970, 0.3030, 0.2920, 0.3052, 0.3057]])\n"
     ]
    }
   ],
   "source": [
    "images = [Image.open(os.path.join(home_dir, \"data\", f\"test_image2.jpg\")).convert(\"RGB\")]\n",
    "texts = [\"black car yellow schoolbus\", \"black schoolbus yellow car\", \"schoolbus right of car\", \"schoolbus left of car\", \"schoolbus with car\", \"schoolbus without car\"]\n",
    "# pretrain_model_dir = r\"D:\\temporary_data\\art_with_shapes\\Logical-CLIP\\src\\models\\patchnet_MANY26_spw_True_batchsize_8_lr_0.001_drop_0.1_lambda_0_singlew_Falsehalfneg_False_fix_objaverse_std_wneg_v5.pt\"\n",
    "# pretrain_model_dir = os.path.join(\"pretrained_models\", \"LOCALMODELNAME\")\n",
    "dcsm_out = get_dcsm_scores(pretrain_model_dir, images, texts)\n",
    "clip_out = get_naive_clip_scores(images,texts, 16)\n",
    "clip_out32 = get_naive_clip_scores(images,texts, 32) #change patch size between 16 & 32\n",
    "\n",
    "print(\"DCSM:\",dcsm_out.detach().cpu(), \"\\nCLIP:\",clip_out.detach().cpu(), \"\\nCLIP 32\", clip_out32.detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python {home_dir}\\src\\training\\main.py --model_save_path models --figure_save_path figures \\\n",
    "    --data_path \"D:\\temporary_data\\art_with_shapes\\Logical-CLIP\\src\\whatsup_vlms\\data\" \\\n",
    "        --epochs 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tester_clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
