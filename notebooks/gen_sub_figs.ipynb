{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: load in 10 images locally, then compute the cosine similarity between their embeddings and the text embedding for \"white cat, black dog\" after loading in CLIP. These values will be the x axis. Then compute the cosine similarity with the images and the text embedding for \"white dog, black cat\", this will be the y axis. Then create a scatterplot where each point corresponds to one image. if the image name has wcbd in it, make the points red. if the image name has wdbc in it, make it blue\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "\n",
    "# Assuming images are in a directory named 'images' in the current working directory\n",
    "image_files = [f\"img{k}_wcbd.png\" for k in range(1,11)] + [f\"img{k}_wdbc.png\" for k in range(1,11)]\n",
    "image_dir = os.getcwd()\n",
    "images = []\n",
    "for i in range(len(image_files)):\n",
    "    try:\n",
    "      img_path = os.path.join(image_dir, image_files[i])\n",
    "      img = Image.open(img_path)\n",
    "      images.append(img)\n",
    "    except Exception as e:\n",
    "      print(f\"Error loading image {image_files[i]}: {e}\")\n",
    "\n",
    "def plot_clip_final_scatter(images,texts):\n",
    "  \"\"\" image_files should have first 10 images corresponding to texts[0], and next 10 images for texts[1]\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  clip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32').to(device)\n",
    "  clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "\n",
    "  # texts = [\"white cat, black dog\", \"white dog, black cat\"]\n",
    "\n",
    "  x_axis = []\n",
    "  y_axis = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for img in images:\n",
    "      image_inputs = clip_processor(images=img, return_tensors=\"pt\").to(device)\n",
    "      image_emb = clip_model.get_image_features(**image_inputs).cpu().numpy()\n",
    "\n",
    "      for text in texts:\n",
    "          text_inputs = clip_processor(text=text, return_tensors=\"pt\").to(device)\n",
    "          text_emb = clip_model.get_text_features(**text_inputs).cpu().numpy()\n",
    "          similarity = cosine_similarity(image_emb, text_emb)[0][0]\n",
    "          if text == texts[0]:#\"white cat, black dog\":\n",
    "            x_axis.append(similarity)\n",
    "          elif text == texts[1]:#\"white dog, black cat\":\n",
    "            y_axis.append(similarity)\n",
    "\n",
    "  # Create scatter plot\n",
    "  plt.figure(figsize=(8, 6))\n",
    "\n",
    "  colors = []\n",
    "  for image_file in image_files:\n",
    "    if \"wcbd\" in image_file:\n",
    "      colors.append('red')\n",
    "    elif \"wdbc\" in image_file:\n",
    "      colors.append('blue')\n",
    "    else:\n",
    "      colors.append('gray')  # Default color for other images\n",
    "\n",
    "  plt.scatter(x_axis, y_axis, c=colors[:len(x_axis)])\n",
    "  plt.xlabel(f\"Cosine Similarity with '{texts[0]}'\")\n",
    "  plt.ylabel(f\"Cosine Similarity with '{texts[1]}\")\n",
    "  plt.title(\"Image Similarity to Text Embeddings\")\n",
    "\n",
    "  # # Add image names as labels (optional)\n",
    "  # for i, image_file in enumerate(image_files[:len(x_axis)]):\n",
    "  #   plt.annotate(image_file, (x_axis[i], y_axis[i]))\n",
    "\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: write a function that takes in a list of 20 PIL images, then imshows the first 10 with a red outline, and then the second 10 with a blue outline.\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def plot_images_with_outlines(images):\n",
    "    \"\"\"\n",
    "    Plots the first 10 images with a red outline, and the next 10 with a blue outline.\n",
    "\n",
    "    Args:\n",
    "        images: A list of 20 PIL Images.\n",
    "    \"\"\"\n",
    "    if len(images) != 20:\n",
    "        raise ValueError(\"The input list must contain exactly 20 PIL Images.\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 10, figsize=(20, 4))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Create a copy of the image to draw on\n",
    "        img_copy = img.copy()\n",
    "        draw = ImageDraw.Draw(img_copy)\n",
    "\n",
    "        # Define outline color\n",
    "        if i < 10:\n",
    "          outline_color = \"red\"\n",
    "        else:\n",
    "          outline_color = \"blue\"\n",
    "        \n",
    "        # Calculate outline width based on image size\n",
    "        width, height = img_copy.size\n",
    "        outline_width = max(int(min(width, height) / 100), 1) # Adjust divisor for thicker/thinner outlines\n",
    "\n",
    "\n",
    "        # Draw the outline\n",
    "        draw.rectangle([(0, 0), (width - 1, height - 1)], outline=outline_color, width=outline_width)\n",
    "\n",
    "        # Display the image\n",
    "        ax.imshow(img_copy)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer, CLIPImageProcessor\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class CrossModalConvNetwork(nn.Module):\n",
    "    def __init__(self, img_seq, text_seq, hidden_dim=128, dropout_prob=0.5):\n",
    "        super(CrossModalConvNetwork, self).__init__()\n",
    "\n",
    "        # 1x1 Convolutions to process cross-modal interactions\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=hidden_dim // 32, kernel_size=(3, 3), padding = 1\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=hidden_dim // 32, out_channels=hidden_dim //32, kernel_size=(3,3), padding = 1\n",
    "        )\n",
    "\n",
    "        # Dropout layers for regularization\n",
    "        self.dropout1 = nn.Dropout2d(p=dropout_prob)\n",
    "        self.dropout2 = nn.Dropout2d(p=dropout_prob)\n",
    "\n",
    "        # Linear layers to reduce the sequence dimension and produce the final output\n",
    "        self.fc1 = nn.Linear(hidden_dim // 32 * text_seq * img_seq, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # Dropout layers for fully connected layers\n",
    "        self.dropout3 = nn.Dropout(p=dropout_prob)\n",
    "        self.dropout4 = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Input tensor of shape (batch_size, batch_size, img_seq, text_seq)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        batch_size1 = x.size(1)\n",
    "\n",
    "        # Add a channel dimension for the 1x1 convolutions: (bs, bs, 1, img_seq, text_seq)\n",
    "        x = x.unsqueeze(2)\n",
    "\n",
    "        # Apply 1x1 convolutions to the image-text interactions\n",
    "        x = x.reshape(batch_size * batch_size1, 1, x.size(3), x.size(4))\n",
    "        x = self.conv1(x)  # Output shape: (bs, bs, hidden_dim, img_seq, text_seq)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout1(x)  # Dropout after first convolution\n",
    "        x = self.conv2(x)  # Output shape: (bs, bs, hidden_dim, img_seq, text_seq)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout2(x)  # Dropout after second convolution\n",
    "\n",
    "        # Flatten the image and text sequence dimensions: (bs, bs, hidden_dim * img_seq * text_seq)\n",
    "        x = x.view(batch_size * batch_size1, -1)\n",
    "\n",
    "        # Apply linear layers to reduce the dimensions and aggregate information\n",
    "        x = self.fc1(x)  # Output shape: (bs, bs, hidden_dim)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout3(x)  # Dropout after first fully connected layer\n",
    "        x = self.fc2(x)  # Output shape: (bs, bs, 1)\n",
    "        x = self.dropout4(x)  # Dropout after final fully connected layer\n",
    "\n",
    "        x = x.view(batch_size, batch_size1, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def standardize(tensor):\n",
    "    mean = tensor.mean()\n",
    "    std = tensor.std()\n",
    "    return (tensor - mean) / std\n",
    "\n",
    "\n",
    "def do_everything(indiv_imgs, texts\n",
    "):\n",
    "    \"\"\"\n",
    "    here, we first try not replacing any of the text tokens. So no <special tokens>\n",
    "    \"\"\"\n",
    "\n",
    "    # STEP 1:\n",
    "    # Load the pre-trained CLIP model and processor\n",
    "    device = \"cuda\"\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "    img_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "    test_special_words = [\n",
    "        \"above\",\n",
    "        \"below\",\n",
    "        \"left\",\n",
    "        \"right\",\n",
    "       \"small\", \"big\", \"not\", \"no\", \"without\", \"but\",\"absent\"]\n",
    "\n",
    "    external_logic_token_embedding = nn.Embedding(\n",
    "        len(test_special_words), 14 * 14 + 1\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        external_logic_token_embedding.weight = nn.Parameter(\n",
    "            (F.normalize(external_logic_token_embedding.weight, p=2, dim=1) + 1) / 3\n",
    "        )\n",
    "\n",
    "    # Set up network n optimizer\n",
    "    image_seq = 14 * 14 + 1\n",
    "    text_seq = 30\n",
    "    hidden_dim = 128\n",
    "\n",
    "    # Initialize the network\n",
    "    task_network = CrossModalConvNetwork(\n",
    "        img_seq=image_seq,\n",
    "        text_seq=text_seq,\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout_prob=0,\n",
    "    ).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    pretrain_model_dir = r\"D:\\Ideal_CLIP\\Ideal-CLIP-DCSM-private\\pretrained_models\\dcsm_obja_ckpt.pt\"\n",
    "\n",
    "    loaded = torch.load(pretrain_model_dir,weights_only=False)\n",
    "    if task_network is not None:\n",
    "        task_network.load_state_dict(loaded[\"model_state_dict\"])\n",
    "\n",
    "    task_network.eval()\n",
    "    output_stack = []\n",
    "    for pp in range(4):\n",
    "      images = [Image.open(ii).convert(\"RGB\") for ii in indiv_imgs[pp*5:pp*5 + 5]]\n",
    "      \n",
    "\n",
    "\n",
    "      # Extract patch and token level embeddings\n",
    "      vision_inputs = img_processor(\n",
    "          images=images, return_tensors=\"pt\"\n",
    "      ).to(device)\n",
    "      vision_outputs = model.vision_model(**vision_inputs)\n",
    "      last_hidden_state = (\n",
    "          vision_outputs.last_hidden_state\n",
    "      )  # Shape: [batch_size, num_patches+1, hidden_size]\n",
    "      post_layernorm = model.vision_model.post_layernorm\n",
    "      lh1 = post_layernorm(last_hidden_state)\n",
    "      lh2 = model.visual_projection(lh1)\n",
    "      image_features = lh2 / torch.norm(lh2, p=2, dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "      text_tokens = tokenizer(\n",
    "          text=texts,\n",
    "          padding=\"max_length\",\n",
    "          max_length=text_seq,\n",
    "          truncation=True,\n",
    "          return_tensors=\"pt\",\n",
    "      ).to(device)\n",
    "      text_features = model.text_model(\n",
    "          **text_tokens\n",
    "      ).last_hidden_state\n",
    "      text_features = model.text_projection(text_features)\n",
    "      text_features = text_features / text_features.norm(\n",
    "          dim=-1, keepdim=True\n",
    "      )\n",
    "\n",
    "      image_features = image_features.unsqueeze(1)\n",
    "      # Shape: (batch_size, 1, iseq, embed_dim)\n",
    "      text_features = text_features.unsqueeze(0)\n",
    "      # Shape: (1, batch_size, tseq, embed_dim)\n",
    "\n",
    "      cossim_mat = torch.einsum(\n",
    "          \"bqie,lpte->bpit\", image_features, text_features\n",
    "      ).to(device)\n",
    "\n",
    "\n",
    "      new_cossim_mat = []\n",
    "      for k in range(len(texts)):\n",
    "          # 0th dim is # of images. 1st dim is # of texts. so we can index in that dimension here.\n",
    "          # we want to replace\n",
    "          # add the special embedding to the correct location in the final output.\n",
    "          sent = texts[k].split(\" \")\n",
    "          # find places where kth sentence has special words.\n",
    "          temp_k_cossim = cossim_mat[:, k, :, :]\n",
    "\n",
    "          # num_valid = len(sent)\n",
    "\n",
    "          for sp_w in test_special_words:\n",
    "              if sp_w in sent:\n",
    "                  special_location = sent.index(sp_w)\n",
    "                  chosen_w = test_special_words.index(sp_w)\n",
    "                  mid = external_logic_token_embedding(\n",
    "                      torch.tensor([chosen_w]).to(model.device)\n",
    "                  )\n",
    "                  mid = mid.unsqueeze(2).repeat(len(images), 1, 1)\n",
    "                  temp_k_cossim = torch.concat(\n",
    "                      [\n",
    "                          temp_k_cossim[:, :, :special_location],\n",
    "                          mid,\n",
    "                          temp_k_cossim[:, :, special_location + 1 :],\n",
    "                      ],\n",
    "                      dim=-1,\n",
    "                  )\n",
    "\n",
    "          new_cossim_mat.append(temp_k_cossim)\n",
    "\n",
    "          # ah ok. I need to broadcast the external logic token through the image dimension.\n",
    "\n",
    "      cossim_mat = torch.stack(new_cossim_mat, dim=1)\n",
    "      # this is now shape bs_i x bs_t x i_seq x t_seq+1 , theoretically.\n",
    "\n",
    "      # let's imshow to confirm\n",
    "      cossim_mat = standardize(cossim_mat)\n",
    "\n",
    "\n",
    "      quicklabels = torch.eye(cossim_mat.shape[0]).to(device)\n",
    "      task_output = (\n",
    "      task_network(cossim_mat).to(device).squeeze()\n",
    "      )  # shape bs x bs x 1.\n",
    "      print(task_output)\n",
    "      output_stack.append(task_output)\n",
    "\n",
    "    return output_stack\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv_imgs = [f\"/content/img{k}_wcbd.png\" for k in range(1,11)] + [f\"img{k}_wdbc.png\" for k in range(1,11)]\n",
    "texts = [\"white dog, black cat\", \"black dog, white cat\"]\n",
    "\n",
    "torch.manual_seed(15)\n",
    "output_stack = do_everything(indiv_imgs, texts)\n",
    "\n",
    "output_tensor = torch.cat(output_stack, 0)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(output_tensor[:10,1].detach().cpu().numpy(), output_tensor[:10,0].detach().cpu().numpy(), c=\"blue\", label=\"wcbd\")\n",
    "plt.scatter(output_tensor[10:,1].detach().cpu().numpy(), output_tensor[10:,0].detach().cpu().numpy(), c=\"red\", label=\"wdbc\")\n",
    "plt.xlabel(\"Cosine Similarity ('white cat, black dog')\")\n",
    "plt.ylabel(\"Cosine Similarity ('white dog, black cat')\")\n",
    "plt.title(\"Image Similarity to Text Embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor = torch.cat(output_stack, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(output_tensor[:10,1].detach().cpu().numpy(), output_tensor[:10,0].detach().cpu().numpy(), c=\"blue\", label=\"wcbd\")\n",
    "plt.scatter(output_tensor[10:,1].detach().cpu().numpy(), output_tensor[10:,0].detach().cpu().numpy(), c=\"red\", label=\"wdbc\")\n",
    "plt.xlabel(\"Cosine Similarity ('white cat, black dog')\")\n",
    "plt.ylabel(\"Cosine Similarity ('white dog, black cat')\")\n",
    "plt.title(\"Image Similarity to Text Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spatial -- synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_tuples = []\n",
    "for jj in range(4):\n",
    "  #the row along which the objects will be\n",
    "  #so camel_0-0_frog_3-0 is camel left of frog\n",
    "  for adder in range(1,4):\n",
    "    for kk in range(2):\n",
    "      if kk+adder < 4:\n",
    "        coord_tuples.append((kk,jj,kk+adder,jj))\n",
    "import random\n",
    "\n",
    "random.shuffle(coord_tuples)\n",
    "len(coord_tuples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tester_clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
